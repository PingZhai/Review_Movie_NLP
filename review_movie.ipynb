{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to read movie review text data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    datadir = \"C:/Users/pingz/MLproject/movie_review/data/aclImdb/\"\n",
    "    flist_pos = [datadir + \"/train/pos/\" + e for e in os.listdir(datadir + \"train/pos/\") if e.endswith(\".txt\")]\n",
    "    flist_neg = [datadir + \"/train/neg/\" + e for e in os.listdir(datadir + \"train/neg/\") if e.endswith(\".txt\")]\n",
    "\n",
    "    review_pos=[]\n",
    "    review_neg=[]\n",
    "    for f in flist_pos:\n",
    "        fo=open(f,encoding=\"utf8\")\n",
    "        review_pos.append(fo.readlines()[0])\n",
    "        fo.close()\n",
    "\n",
    "    for f in flist_neg:\n",
    "        fo=open(f,encoding=\"utf8\")\n",
    "        review_neg.append(fo.readlines()[0])\n",
    "        fo.close()\n",
    "    train_data = pd.concat([pd.DataFrame({\"review\":review_pos, \"label\":1, \"file\":flist_pos}),\\\n",
    "        pd.DataFrame({\"review\":review_neg, \"label\":0, \"file\":flist_neg})], ignore_index=True).sample(frac=1, random_state=1)\n",
    "\n",
    "    flist_pos = [datadir + \"/test/pos/\" + e for e in os.listdir(datadir + \"test/pos/\") if e.endswith(\".txt\")]\n",
    "    flist_neg = [datadir + \"/test/neg/\" + e for e in os.listdir(datadir + \"test/neg/\") if e.endswith(\".txt\")]\n",
    "\n",
    "    review_pos=[]\n",
    "    review_neg=[]\n",
    "    for f in flist_pos:\n",
    "        fo=open(f,encoding=\"utf8\")\n",
    "        review_pos.append(fo.readlines()[0])\n",
    "        fo.close()\n",
    "\n",
    "    for f in flist_neg:\n",
    "        fo=open(f,encoding=\"utf8\")\n",
    "        review_neg.append(fo.readlines()[0])\n",
    "        fo.close()\n",
    "    test_data = pd.concat([pd.DataFrame({\"review\":review_pos, \"label\":1, \"file\":flist_pos}),\\\n",
    "        pd.DataFrame({\"review\":review_neg, \"label\":0, \"file\":flist_neg})], ignore_index=True).sample(frac=1, random_state=1)\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=read_data() #read train and test data into dataframe. Positive reviews are marked as 1, and negative 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 12500, 1: 12500})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21492</td>\n",
       "      <td>I have copy of this on VHS, I think they (The ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:/Users/pingz/MLproject/movie_review/data/acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9488</td>\n",
       "      <td>After several extremely well ratings to the po...</td>\n",
       "      <td>1</td>\n",
       "      <td>C:/Users/pingz/MLproject/movie_review/data/acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16933</td>\n",
       "      <td>I still don't know why I forced myself to sit ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:/Users/pingz/MLproject/movie_review/data/acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12604</td>\n",
       "      <td>Mt little sister and I are self-proclaimed hor...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:/Users/pingz/MLproject/movie_review/data/acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8222</td>\n",
       "      <td>I have personally seen many Disney movies in m...</td>\n",
       "      <td>1</td>\n",
       "      <td>C:/Users/pingz/MLproject/movie_review/data/acl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label  \\\n",
       "21492  I have copy of this on VHS, I think they (The ...      0   \n",
       "9488   After several extremely well ratings to the po...      1   \n",
       "16933  I still don't know why I forced myself to sit ...      0   \n",
       "12604  Mt little sister and I are self-proclaimed hor...      0   \n",
       "8222   I have personally seen many Disney movies in m...      1   \n",
       "\n",
       "                                                    file  \n",
       "21492  C:/Users/pingz/MLproject/movie_review/data/acl...  \n",
       "9488   C:/Users/pingz/MLproject/movie_review/data/acl...  \n",
       "16933  C:/Users/pingz/MLproject/movie_review/data/acl...  \n",
       "12604  C:/Users/pingz/MLproject/movie_review/data/acl...  \n",
       "8222   C:/Users/pingz/MLproject/movie_review/data/acl...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Counter(train_data['label']))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is balanced since there are 12500 cases of negative reviews and 12500 cases of positive reviews. Therefore, we can use accuracy to evaluate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review: After several extremely well ratings to the point of SUPERB, I was extremely pleased with the film. The film was dark, moving, the anger, the pain, the guilt and a very extremely convincing demon.<br /><br />I had initially expected to see many special effects, and like a lover's caress, it blew me away with the subtlety and the rightness of it. Brian, I am again blown away with your artistry with the telling of the story and your care of the special effects. You will go a long way, my friend. I will definitely be the president of your fan club.<br /><br />Eric Etebari, the best actor award, was the number one choice. You made Jr. Lopez look like a child compared to Kasadya. :) <br /><br />Overall, the acting, story line, the high quality filming and awesome effects, it was fantastic. I just wish it were longer. I am looking forward to The Dreamless with extremely high expectations.\n",
      "\n",
      "Negative review: I have copy of this on VHS, I think they (The television networks) should play this every year for the next twenty years. So that we don't forget what was and that we remember not to do the same mistakes again. Like putting some people in the director's chair, where they don't belong. This movie Rappin' is like a vaudevillian musical, for those who can't sing, or act. This movie is as much fun as trying to teach the 'blind' to drive a city bus.<br /><br />John Hood, (Peebles) has just got out of prison and he's headed back to the old neighborhood. In serving time for an all-to-nice crime of necessity, of course. John heads back onto the old street and is greeted by kids dogs old ladies and his peer homeys as they dance and sing all along the way.<br /><br />I would recommend this if I was sentimental, or if in truth someone was smoking medicinal pot prescribed by a doctor for glaucoma. Either way this is a poorly directed, scripted, acted and even produced (I never thought I'd sat that) satire of ghetto life with the 'Hood'. Although, I think the redeeming part of the story, through the wannabe gang fight sequences and the dance numbers, his friends care about their neighbors and want to save the ghetto from being torn down and cleaned up. <br /><br />Forget Sonny spoon, Mario could have won an Oscar for that in comparison to this Rap. Oh well if you find yourself wanting to laugh yourself silly and three-quarters embarrassed, be sure to drink first. <br /><br />And please, watch responsibly. (No stars, better luck next time!)\n"
     ]
    }
   ],
   "source": [
    "print('Positive review: '+train_data['review'].iloc[1]+'\\n')\n",
    "print('Negative review: '+train_data['review'].iloc[0]) #check two examples of positive and negative reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove HTML codes, replace special characters with space, conver to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['review'] = train_data['review'].apply(lambda r: BeautifulSoup(r, 'html.parser').get_text())\n",
    "train_data['review'].replace(to_replace ='[^a-z A-Z]', value = '', regex = True, inplace = True)\n",
    "train_data['review'] = train_data['review'].str.lower()  #convert to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Remove stopwords, duplicate words and grouping together the different forms of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pingz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "estopwords = set(stopwords.words('english')).union([\"thats\",\"weve\",\"dont\",\"lets\",\"youre\",\"im\",\"thi\",\"ha\",\\\n",
    "    \"wa\",\"st\",\"ask\",\"want\",\"like\",\"thank\",\"know\",\"susan\",\"ryan\",\"say\",\"got\",\"ought\",\"ive\",\"theyre\"])  #get the English stop words\n",
    "train_data['review'] = train_data['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in set(x.split()) if word not in estopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the words and pad the tokens with fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABS = 5000\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCABS)\n",
    "tokenizer.fit_on_texts(pd.concat([train_data['review']]))\n",
    "x_train = tokenizer.texts_to_sequences(train_data['review'])\n",
    "MAX_LEN = max([len(i) for i in x_train])\n",
    "vocab_size = MAX_VOCABS + 1\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=MAX_LEN, value=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train data into train data and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(train_data['label'])\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with embedd_dim=16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pingz\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\pingz\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\pingz\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/75\n",
      "20000/20000 - 1s - loss: 0.6928 - acc: 0.5289 - val_loss: 0.6922 - val_acc: 0.5002\n",
      "Epoch 2/75\n",
      "20000/20000 - 1s - loss: 0.6908 - acc: 0.5705 - val_loss: 0.6892 - val_acc: 0.5112\n",
      "Epoch 3/75\n",
      "20000/20000 - 1s - loss: 0.6857 - acc: 0.6731 - val_loss: 0.6815 - val_acc: 0.7340\n",
      "Epoch 4/75\n",
      "20000/20000 - 1s - loss: 0.6741 - acc: 0.7949 - val_loss: 0.6663 - val_acc: 0.8002\n",
      "Epoch 5/75\n",
      "20000/20000 - 1s - loss: 0.6529 - acc: 0.8055 - val_loss: 0.6409 - val_acc: 0.7702\n",
      "Epoch 6/75\n",
      "20000/20000 - 1s - loss: 0.6212 - acc: 0.8135 - val_loss: 0.6062 - val_acc: 0.8182\n",
      "Epoch 7/75\n",
      "20000/20000 - 1s - loss: 0.5805 - acc: 0.8364 - val_loss: 0.5669 - val_acc: 0.8060\n",
      "Epoch 8/75\n",
      "20000/20000 - 1s - loss: 0.5371 - acc: 0.8368 - val_loss: 0.5232 - val_acc: 0.8346\n",
      "Epoch 9/75\n",
      "20000/20000 - 1s - loss: 0.4937 - acc: 0.8483 - val_loss: 0.4848 - val_acc: 0.8476\n",
      "Epoch 10/75\n",
      "20000/20000 - 1s - loss: 0.4543 - acc: 0.8566 - val_loss: 0.4509 - val_acc: 0.8522\n",
      "Epoch 11/75\n",
      "20000/20000 - 1s - loss: 0.4200 - acc: 0.8651 - val_loss: 0.4227 - val_acc: 0.8544\n",
      "Epoch 12/75\n",
      "20000/20000 - 1s - loss: 0.3918 - acc: 0.8666 - val_loss: 0.3997 - val_acc: 0.8584\n",
      "Epoch 13/75\n",
      "20000/20000 - 1s - loss: 0.3676 - acc: 0.8723 - val_loss: 0.3835 - val_acc: 0.8604\n",
      "Epoch 14/75\n",
      "20000/20000 - 1s - loss: 0.3508 - acc: 0.8735 - val_loss: 0.3666 - val_acc: 0.8640\n",
      "Epoch 15/75\n",
      "20000/20000 - 1s - loss: 0.3315 - acc: 0.8798 - val_loss: 0.3551 - val_acc: 0.8624\n",
      "Epoch 16/75\n",
      "20000/20000 - 1s - loss: 0.3164 - acc: 0.8831 - val_loss: 0.3438 - val_acc: 0.8656\n",
      "Epoch 17/75\n",
      "20000/20000 - 1s - loss: 0.3031 - acc: 0.8874 - val_loss: 0.3344 - val_acc: 0.8688\n",
      "Epoch 18/75\n",
      "20000/20000 - 1s - loss: 0.2917 - acc: 0.8917 - val_loss: 0.3272 - val_acc: 0.8702\n",
      "Epoch 19/75\n",
      "20000/20000 - 1s - loss: 0.2817 - acc: 0.8946 - val_loss: 0.3214 - val_acc: 0.8716\n",
      "Epoch 20/75\n",
      "20000/20000 - 1s - loss: 0.2735 - acc: 0.8974 - val_loss: 0.3178 - val_acc: 0.8756\n",
      "Epoch 21/75\n",
      "20000/20000 - 1s - loss: 0.2653 - acc: 0.9009 - val_loss: 0.3131 - val_acc: 0.8770\n",
      "Epoch 22/75\n",
      "20000/20000 - 1s - loss: 0.2580 - acc: 0.9030 - val_loss: 0.3128 - val_acc: 0.8760\n",
      "Epoch 23/75\n",
      "20000/20000 - 1s - loss: 0.2525 - acc: 0.9057 - val_loss: 0.3069 - val_acc: 0.8752\n",
      "Epoch 24/75\n",
      "20000/20000 - 1s - loss: 0.2459 - acc: 0.9090 - val_loss: 0.3081 - val_acc: 0.8764\n",
      "Epoch 25/75\n",
      "20000/20000 - 1s - loss: 0.2409 - acc: 0.9089 - val_loss: 0.3027 - val_acc: 0.8786\n",
      "Epoch 26/75\n",
      "20000/20000 - 1s - loss: 0.2359 - acc: 0.9121 - val_loss: 0.3029 - val_acc: 0.8770\n",
      "Epoch 27/75\n",
      "20000/20000 - 1s - loss: 0.2321 - acc: 0.9125 - val_loss: 0.3002 - val_acc: 0.8784\n",
      "Epoch 28/75\n",
      "20000/20000 - 1s - loss: 0.2267 - acc: 0.9158 - val_loss: 0.2993 - val_acc: 0.8778\n",
      "Epoch 29/75\n",
      "20000/20000 - 1s - loss: 0.2237 - acc: 0.9162 - val_loss: 0.2998 - val_acc: 0.8812\n",
      "Epoch 30/75\n",
      "20000/20000 - 1s - loss: 0.2197 - acc: 0.9183 - val_loss: 0.3043 - val_acc: 0.8780\n",
      "Epoch 31/75\n",
      "20000/20000 - 1s - loss: 0.2168 - acc: 0.9200 - val_loss: 0.2981 - val_acc: 0.8800\n",
      "Epoch 32/75\n",
      "20000/20000 - 1s - loss: 0.2124 - acc: 0.9220 - val_loss: 0.2976 - val_acc: 0.8792\n",
      "Epoch 33/75\n",
      "20000/20000 - 1s - loss: 0.2097 - acc: 0.9229 - val_loss: 0.3057 - val_acc: 0.8772\n",
      "Epoch 34/75\n",
      "20000/20000 - 1s - loss: 0.2072 - acc: 0.9236 - val_loss: 0.3028 - val_acc: 0.8814\n",
      "Epoch 35/75\n",
      "20000/20000 - 1s - loss: 0.2041 - acc: 0.9236 - val_loss: 0.2995 - val_acc: 0.8802\n",
      "Epoch 36/75\n",
      "20000/20000 - 1s - loss: 0.2004 - acc: 0.9253 - val_loss: 0.2990 - val_acc: 0.8796\n",
      "Epoch 37/75\n",
      "20000/20000 - 1s - loss: 0.1999 - acc: 0.9252 - val_loss: 0.2998 - val_acc: 0.8796\n",
      "Epoch 38/75\n",
      "20000/20000 - 1s - loss: 0.1953 - acc: 0.9273 - val_loss: 0.3015 - val_acc: 0.8800\n",
      "Epoch 39/75\n",
      "20000/20000 - 1s - loss: 0.1925 - acc: 0.9288 - val_loss: 0.3011 - val_acc: 0.8790\n",
      "Epoch 40/75\n",
      "20000/20000 - 1s - loss: 0.1905 - acc: 0.9307 - val_loss: 0.3017 - val_acc: 0.8798\n",
      "Epoch 41/75\n",
      "20000/20000 - 1s - loss: 0.1882 - acc: 0.9301 - val_loss: 0.3030 - val_acc: 0.8780\n",
      "Epoch 42/75\n",
      "20000/20000 - 1s - loss: 0.1860 - acc: 0.9312 - val_loss: 0.3038 - val_acc: 0.8786\n",
      "Epoch 43/75\n",
      "20000/20000 - 1s - loss: 0.1839 - acc: 0.9327 - val_loss: 0.3058 - val_acc: 0.8784\n",
      "Epoch 44/75\n",
      "20000/20000 - 1s - loss: 0.1820 - acc: 0.9331 - val_loss: 0.3104 - val_acc: 0.8776\n",
      "Epoch 45/75\n",
      "20000/20000 - 1s - loss: 0.1815 - acc: 0.9326 - val_loss: 0.3071 - val_acc: 0.8784\n",
      "Epoch 46/75\n",
      "20000/20000 - 1s - loss: 0.1778 - acc: 0.9348 - val_loss: 0.3078 - val_acc: 0.8778\n",
      "Epoch 47/75\n",
      "20000/20000 - 1s - loss: 0.1756 - acc: 0.9364 - val_loss: 0.3092 - val_acc: 0.8770\n",
      "Epoch 48/75\n",
      "20000/20000 - 1s - loss: 0.1741 - acc: 0.9370 - val_loss: 0.3105 - val_acc: 0.8770\n",
      "Epoch 49/75\n",
      "20000/20000 - 1s - loss: 0.1734 - acc: 0.9360 - val_loss: 0.3121 - val_acc: 0.8772\n",
      "Epoch 50/75\n",
      "20000/20000 - 1s - loss: 0.1702 - acc: 0.9383 - val_loss: 0.3139 - val_acc: 0.8782\n",
      "Epoch 51/75\n",
      "20000/20000 - 1s - loss: 0.1697 - acc: 0.9376 - val_loss: 0.3159 - val_acc: 0.8776\n",
      "Epoch 52/75\n",
      "20000/20000 - 1s - loss: 0.1675 - acc: 0.9389 - val_loss: 0.3169 - val_acc: 0.8792\n",
      "Epoch 53/75\n",
      "20000/20000 - 1s - loss: 0.1653 - acc: 0.9403 - val_loss: 0.3186 - val_acc: 0.8780\n",
      "Epoch 54/75\n",
      "20000/20000 - 1s - loss: 0.1638 - acc: 0.9406 - val_loss: 0.3200 - val_acc: 0.8770\n",
      "Epoch 55/75\n",
      "20000/20000 - 1s - loss: 0.1624 - acc: 0.9416 - val_loss: 0.3221 - val_acc: 0.8774\n",
      "Epoch 56/75\n",
      "20000/20000 - 1s - loss: 0.1624 - acc: 0.9405 - val_loss: 0.3261 - val_acc: 0.8756\n",
      "Epoch 57/75\n",
      "20000/20000 - 1s - loss: 0.1599 - acc: 0.9416 - val_loss: 0.3249 - val_acc: 0.8768\n",
      "Epoch 58/75\n",
      "20000/20000 - 1s - loss: 0.1582 - acc: 0.9440 - val_loss: 0.3272 - val_acc: 0.8772\n",
      "Epoch 59/75\n",
      "20000/20000 - 1s - loss: 0.1564 - acc: 0.9446 - val_loss: 0.3291 - val_acc: 0.8768\n",
      "Epoch 60/75\n",
      "20000/20000 - 1s - loss: 0.1551 - acc: 0.9434 - val_loss: 0.3314 - val_acc: 0.8764\n",
      "Epoch 61/75\n",
      "20000/20000 - 1s - loss: 0.1539 - acc: 0.9451 - val_loss: 0.3326 - val_acc: 0.8756\n",
      "Epoch 62/75\n",
      "20000/20000 - 1s - loss: 0.1521 - acc: 0.9459 - val_loss: 0.3351 - val_acc: 0.8754\n",
      "Epoch 63/75\n",
      "20000/20000 - 1s - loss: 0.1511 - acc: 0.9463 - val_loss: 0.3405 - val_acc: 0.8724\n",
      "Epoch 64/75\n",
      "20000/20000 - 1s - loss: 0.1508 - acc: 0.9459 - val_loss: 0.3389 - val_acc: 0.8744\n",
      "Epoch 65/75\n",
      "20000/20000 - 1s - loss: 0.1487 - acc: 0.9474 - val_loss: 0.3408 - val_acc: 0.8742\n",
      "Epoch 66/75\n",
      "20000/20000 - 1s - loss: 0.1496 - acc: 0.9470 - val_loss: 0.3429 - val_acc: 0.8760\n",
      "Epoch 67/75\n",
      "20000/20000 - 1s - loss: 0.1471 - acc: 0.9480 - val_loss: 0.3448 - val_acc: 0.8750\n",
      "Epoch 68/75\n",
      "20000/20000 - 1s - loss: 0.1461 - acc: 0.9477 - val_loss: 0.3488 - val_acc: 0.8734\n",
      "Epoch 69/75\n",
      "20000/20000 - 1s - loss: 0.1452 - acc: 0.9481 - val_loss: 0.3490 - val_acc: 0.8738\n",
      "Epoch 70/75\n",
      "20000/20000 - 1s - loss: 0.1429 - acc: 0.9485 - val_loss: 0.3520 - val_acc: 0.8732\n",
      "Epoch 71/75\n",
      "20000/20000 - 1s - loss: 0.1424 - acc: 0.9495 - val_loss: 0.3558 - val_acc: 0.8712\n",
      "Epoch 72/75\n",
      "20000/20000 - 1s - loss: 0.1411 - acc: 0.9503 - val_loss: 0.3548 - val_acc: 0.8738\n",
      "Epoch 73/75\n",
      "20000/20000 - 1s - loss: 0.1402 - acc: 0.9506 - val_loss: 0.3587 - val_acc: 0.8716\n",
      "Epoch 74/75\n",
      "20000/20000 - 1s - loss: 0.1380 - acc: 0.9520 - val_loss: 0.3589 - val_acc: 0.8752\n",
      "Epoch 75/75\n",
      "20000/20000 - 1s - loss: 0.1378 - acc: 0.9521 - val_loss: 0.3611 - val_acc: 0.8738\n"
     ]
    }
   ],
   "source": [
    "embedd_dim = 16\n",
    "model = keras.Sequential([keras.layers.Embedding(vocab_size + 1, embedd_dim), keras.layers.GlobalAveragePooling1D(),\\\n",
    "                          keras.layers.Dense(16, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=75,  batch_size=512, verbose = 2, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test data to test the performance of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for test data with embedd_dim=16 is  0.86344\n"
     ]
    }
   ],
   "source": [
    "test_data['review'] = test_data['review'].apply(lambda r: BeautifulSoup(r, 'html.parser').get_text())\n",
    "test_data['review'].replace(to_replace ='[^a-z A-Z]', value = '', regex = True, inplace = True)\n",
    "test_data['review'] = test_data['review'].str.lower()  #convert to lower case\n",
    "test_data['review'] = test_data['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in set(x.split()) if word not in estopwords]))\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(test_data['review'])\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=MAX_LEN, value=vocab_size)\n",
    "y_test=test_data['label']\n",
    "y_test_predict_prob=model.predict(x_test)\n",
    "y_test_predict=[1 if e >=0.5 else 0 for e in y_test_predict_prob]\n",
    "print('accuracy score for test data with embedd_dim=16 is ',accuracy_score(y_test_predict,y_test))\n",
    "df_test_label=pd.DataFrame({\"test_label_true\":list(y_test), \"test_label_predict\":y_test_predict,\"test_label_predict_probability\":list(y_test_predict_prob[:,0])})\n",
    "df_test_label.to_csv('test_label_true_predict.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/75\n",
      "20000/20000 - 1s - loss: 0.6928 - acc: 0.5283 - val_loss: 0.6922 - val_acc: 0.5006\n",
      "Epoch 2/75\n",
      "20000/20000 - 1s - loss: 0.6906 - acc: 0.5751 - val_loss: 0.6883 - val_acc: 0.8234\n",
      "Epoch 3/75\n",
      "20000/20000 - 1s - loss: 0.6835 - acc: 0.6953 - val_loss: 0.6769 - val_acc: 0.8060\n",
      "Epoch 4/75\n",
      "20000/20000 - 1s - loss: 0.6642 - acc: 0.8030 - val_loss: 0.6489 - val_acc: 0.8150\n",
      "Epoch 5/75\n",
      "20000/20000 - 1s - loss: 0.6234 - acc: 0.8208 - val_loss: 0.5980 - val_acc: 0.8280\n",
      "Epoch 6/75\n",
      "20000/20000 - 1s - loss: 0.5601 - acc: 0.8304 - val_loss: 0.5309 - val_acc: 0.8356\n",
      "Epoch 7/75\n",
      "20000/20000 - 1s - loss: 0.4903 - acc: 0.8469 - val_loss: 0.4693 - val_acc: 0.8416\n",
      "Epoch 8/75\n",
      "20000/20000 - 1s - loss: 0.4338 - acc: 0.8489 - val_loss: 0.4283 - val_acc: 0.8462\n",
      "Epoch 9/75\n",
      "20000/20000 - 1s - loss: 0.3892 - acc: 0.8650 - val_loss: 0.3915 - val_acc: 0.8560\n",
      "Epoch 10/75\n",
      "20000/20000 - 1s - loss: 0.3552 - acc: 0.8712 - val_loss: 0.3674 - val_acc: 0.8602\n",
      "Epoch 11/75\n",
      "20000/20000 - 1s - loss: 0.3293 - acc: 0.8783 - val_loss: 0.3523 - val_acc: 0.8648\n",
      "Epoch 12/75\n",
      "20000/20000 - 1s - loss: 0.3099 - acc: 0.8826 - val_loss: 0.3363 - val_acc: 0.8688\n",
      "Epoch 13/75\n",
      "20000/20000 - 1s - loss: 0.2919 - acc: 0.8887 - val_loss: 0.3291 - val_acc: 0.8720\n",
      "Epoch 14/75\n",
      "20000/20000 - 1s - loss: 0.2775 - acc: 0.8939 - val_loss: 0.3201 - val_acc: 0.8696\n",
      "Epoch 15/75\n",
      "20000/20000 - 1s - loss: 0.2668 - acc: 0.8995 - val_loss: 0.3151 - val_acc: 0.8758\n",
      "Epoch 16/75\n",
      "20000/20000 - 1s - loss: 0.2564 - acc: 0.9026 - val_loss: 0.3097 - val_acc: 0.8786\n",
      "Epoch 17/75\n",
      "20000/20000 - 1s - loss: 0.2476 - acc: 0.9064 - val_loss: 0.3108 - val_acc: 0.8772\n",
      "Epoch 18/75\n",
      "20000/20000 - 1s - loss: 0.2415 - acc: 0.9072 - val_loss: 0.3037 - val_acc: 0.8780\n",
      "Epoch 19/75\n",
      "20000/20000 - 1s - loss: 0.2345 - acc: 0.9114 - val_loss: 0.3008 - val_acc: 0.8780\n",
      "Epoch 20/75\n",
      "20000/20000 - 1s - loss: 0.2285 - acc: 0.9130 - val_loss: 0.3001 - val_acc: 0.8792\n",
      "Epoch 21/75\n",
      "20000/20000 - 1s - loss: 0.2219 - acc: 0.9172 - val_loss: 0.2998 - val_acc: 0.8794\n",
      "Epoch 22/75\n",
      "20000/20000 - 1s - loss: 0.2172 - acc: 0.9192 - val_loss: 0.2991 - val_acc: 0.8786\n",
      "Epoch 23/75\n",
      "20000/20000 - 1s - loss: 0.2121 - acc: 0.9208 - val_loss: 0.2993 - val_acc: 0.8792\n",
      "Epoch 24/75\n",
      "20000/20000 - 1s - loss: 0.2078 - acc: 0.9225 - val_loss: 0.3013 - val_acc: 0.8788\n",
      "Epoch 25/75\n",
      "20000/20000 - 1s - loss: 0.2049 - acc: 0.9237 - val_loss: 0.3006 - val_acc: 0.8806\n",
      "Epoch 26/75\n",
      "20000/20000 - 1s - loss: 0.1997 - acc: 0.9265 - val_loss: 0.3005 - val_acc: 0.8798\n",
      "Epoch 27/75\n",
      "20000/20000 - 1s - loss: 0.1986 - acc: 0.9252 - val_loss: 0.3018 - val_acc: 0.8796\n",
      "Epoch 28/75\n",
      "20000/20000 - 1s - loss: 0.1944 - acc: 0.9280 - val_loss: 0.3042 - val_acc: 0.8806\n",
      "Epoch 29/75\n",
      "20000/20000 - 1s - loss: 0.1919 - acc: 0.9273 - val_loss: 0.3062 - val_acc: 0.8792\n",
      "Epoch 30/75\n",
      "20000/20000 - 1s - loss: 0.1867 - acc: 0.9309 - val_loss: 0.3094 - val_acc: 0.8778\n",
      "Epoch 31/75\n",
      "20000/20000 - 1s - loss: 0.1850 - acc: 0.9312 - val_loss: 0.3069 - val_acc: 0.8784\n",
      "Epoch 32/75\n",
      "20000/20000 - 1s - loss: 0.1831 - acc: 0.9316 - val_loss: 0.3081 - val_acc: 0.8778\n",
      "Epoch 33/75\n",
      "20000/20000 - 1s - loss: 0.1784 - acc: 0.9352 - val_loss: 0.3170 - val_acc: 0.8748\n",
      "Epoch 34/75\n",
      "20000/20000 - 1s - loss: 0.1769 - acc: 0.9345 - val_loss: 0.3125 - val_acc: 0.8778\n",
      "Epoch 35/75\n",
      "20000/20000 - 1s - loss: 0.1739 - acc: 0.9348 - val_loss: 0.3152 - val_acc: 0.8776\n",
      "Epoch 36/75\n",
      "20000/20000 - 1s - loss: 0.1716 - acc: 0.9377 - val_loss: 0.3207 - val_acc: 0.8746\n",
      "Epoch 37/75\n",
      "20000/20000 - 1s - loss: 0.1690 - acc: 0.9377 - val_loss: 0.3228 - val_acc: 0.8742\n",
      "Epoch 38/75\n",
      "20000/20000 - 1s - loss: 0.1659 - acc: 0.9391 - val_loss: 0.3198 - val_acc: 0.8770\n",
      "Epoch 39/75\n",
      "20000/20000 - 1s - loss: 0.1636 - acc: 0.9402 - val_loss: 0.3244 - val_acc: 0.8764\n",
      "Epoch 40/75\n",
      "20000/20000 - 1s - loss: 0.1627 - acc: 0.9402 - val_loss: 0.3307 - val_acc: 0.8748\n",
      "Epoch 41/75\n",
      "20000/20000 - 1s - loss: 0.1624 - acc: 0.9398 - val_loss: 0.3275 - val_acc: 0.8746\n",
      "Epoch 42/75\n",
      "20000/20000 - 1s - loss: 0.1593 - acc: 0.9419 - val_loss: 0.3312 - val_acc: 0.8738\n",
      "Epoch 43/75\n",
      "20000/20000 - 1s - loss: 0.1562 - acc: 0.9436 - val_loss: 0.3347 - val_acc: 0.8722\n",
      "Epoch 44/75\n",
      "20000/20000 - 1s - loss: 0.1545 - acc: 0.9446 - val_loss: 0.3342 - val_acc: 0.8750\n",
      "Epoch 45/75\n",
      "20000/20000 - 1s - loss: 0.1522 - acc: 0.9451 - val_loss: 0.3369 - val_acc: 0.8746\n",
      "Epoch 46/75\n",
      "20000/20000 - 1s - loss: 0.1528 - acc: 0.9446 - val_loss: 0.3395 - val_acc: 0.8748\n",
      "Epoch 47/75\n",
      "20000/20000 - 1s - loss: 0.1520 - acc: 0.9445 - val_loss: 0.3425 - val_acc: 0.8746\n",
      "Epoch 48/75\n",
      "20000/20000 - 1s - loss: 0.1472 - acc: 0.9471 - val_loss: 0.3455 - val_acc: 0.8738\n",
      "Epoch 49/75\n",
      "20000/20000 - 1s - loss: 0.1471 - acc: 0.9474 - val_loss: 0.3497 - val_acc: 0.8726\n",
      "Epoch 50/75\n",
      "20000/20000 - 1s - loss: 0.1439 - acc: 0.9493 - val_loss: 0.3505 - val_acc: 0.8742\n",
      "Epoch 51/75\n",
      "20000/20000 - 1s - loss: 0.1422 - acc: 0.9495 - val_loss: 0.3532 - val_acc: 0.8736\n",
      "Epoch 52/75\n",
      "20000/20000 - 1s - loss: 0.1405 - acc: 0.9517 - val_loss: 0.3605 - val_acc: 0.8694\n",
      "Epoch 53/75\n",
      "20000/20000 - 1s - loss: 0.1415 - acc: 0.9494 - val_loss: 0.3601 - val_acc: 0.8726\n",
      "Epoch 54/75\n",
      "20000/20000 - 1s - loss: 0.1405 - acc: 0.9513 - val_loss: 0.3648 - val_acc: 0.8704\n",
      "Epoch 55/75\n",
      "20000/20000 - 1s - loss: 0.1364 - acc: 0.9523 - val_loss: 0.3659 - val_acc: 0.8730\n",
      "Epoch 56/75\n",
      "20000/20000 - 1s - loss: 0.1363 - acc: 0.9524 - val_loss: 0.3685 - val_acc: 0.8716\n",
      "Epoch 57/75\n",
      "20000/20000 - 1s - loss: 0.1350 - acc: 0.9527 - val_loss: 0.3726 - val_acc: 0.8706\n",
      "Epoch 58/75\n",
      "20000/20000 - 1s - loss: 0.1324 - acc: 0.9547 - val_loss: 0.3746 - val_acc: 0.8716\n",
      "Epoch 59/75\n",
      "20000/20000 - 1s - loss: 0.1315 - acc: 0.9554 - val_loss: 0.3819 - val_acc: 0.8660\n",
      "Epoch 60/75\n",
      "20000/20000 - 1s - loss: 0.1309 - acc: 0.9549 - val_loss: 0.3845 - val_acc: 0.8666\n",
      "Epoch 61/75\n",
      "20000/20000 - 1s - loss: 0.1311 - acc: 0.9541 - val_loss: 0.3839 - val_acc: 0.8698\n",
      "Epoch 62/75\n",
      "20000/20000 - 1s - loss: 0.1272 - acc: 0.9564 - val_loss: 0.3902 - val_acc: 0.8668\n",
      "Epoch 63/75\n",
      "20000/20000 - 1s - loss: 0.1266 - acc: 0.9567 - val_loss: 0.3937 - val_acc: 0.8652\n",
      "Epoch 64/75\n",
      "20000/20000 - 1s - loss: 0.1280 - acc: 0.9553 - val_loss: 0.3951 - val_acc: 0.8682\n",
      "Epoch 65/75\n",
      "20000/20000 - 1s - loss: 0.1251 - acc: 0.9575 - val_loss: 0.3967 - val_acc: 0.8672\n",
      "Epoch 66/75\n",
      "20000/20000 - 1s - loss: 0.1225 - acc: 0.9606 - val_loss: 0.4024 - val_acc: 0.8668\n",
      "Epoch 67/75\n",
      "20000/20000 - 1s - loss: 0.1217 - acc: 0.9601 - val_loss: 0.4053 - val_acc: 0.8664\n",
      "Epoch 68/75\n",
      "20000/20000 - 1s - loss: 0.1214 - acc: 0.9593 - val_loss: 0.4094 - val_acc: 0.8652\n",
      "Epoch 69/75\n",
      "20000/20000 - 1s - loss: 0.1193 - acc: 0.9617 - val_loss: 0.4162 - val_acc: 0.8644\n",
      "Epoch 70/75\n",
      "20000/20000 - 1s - loss: 0.1223 - acc: 0.9582 - val_loss: 0.4238 - val_acc: 0.8636\n",
      "Epoch 71/75\n",
      "20000/20000 - 1s - loss: 0.1185 - acc: 0.9611 - val_loss: 0.4190 - val_acc: 0.8638\n",
      "Epoch 72/75\n",
      "20000/20000 - 1s - loss: 0.1165 - acc: 0.9618 - val_loss: 0.4254 - val_acc: 0.8646\n",
      "Epoch 73/75\n",
      "20000/20000 - 1s - loss: 0.1150 - acc: 0.9628 - val_loss: 0.4290 - val_acc: 0.8646\n",
      "Epoch 74/75\n",
      "20000/20000 - 1s - loss: 0.1162 - acc: 0.9615 - val_loss: 0.4291 - val_acc: 0.8634\n",
      "Epoch 75/75\n",
      "20000/20000 - 1s - loss: 0.1128 - acc: 0.9633 - val_loss: 0.4332 - val_acc: 0.8620\n"
     ]
    }
   ],
   "source": [
    "embedd_dim = 32\n",
    "model32 = keras.Sequential([keras.layers.Embedding(vocab_size + 1, embedd_dim), keras.layers.GlobalAveragePooling1D(),\\\n",
    "                          keras.layers.Dense(32, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model32.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history = model32.fit(x_train, y_train, epochs=75,  batch_size=512, verbose = 2, validation_data = (x_val, y_val))\n",
    "y_test_predict_prob=model32.predict(x_test)\n",
    "y_test_predict=[1 if e >=0.5 else 0 for e in y_test_predict_prob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for test data with embedd_dim=32 is  0.85388\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score for test data with embedd_dim=32 is ',accuracy_score(y_test_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that when embedd_dim=16 is used, the model performance is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy for training set is increasing with Epoch, the accuracy for validation dataset starts to decrease after Epoch 18, and the model starts to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
